{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS439 Student Performance Project\n",
    "\n",
    "Dataset: `student-por.csv` (Portuguese course; 649 rows, 33 columns). Target: `G3` (final grade). The notebook loads the data, inspects schema/quality, and sets up space for EDA and baseline modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126d7674",
   "metadata": {},
   "source": [
    "## Plan, success criteria, course alignment\n",
    "- Scope: predict `G3` from early-term grades and student attributes; prioritize interpretability and fairness checks.\n",
    "- Timeline: (1) EDA/cleaning, (2) feature engineering, (3) models/baselines, (4) diagnostics + fairness slices, (5) interpretation/report write-up.\n",
    "- Success: MAE ≤ 2.5–3.0 grade points and R² ≥ 0.75 on held-out split; stable CV scores; no large error gaps across `sex`/`school`.\n",
    "- Course tie-in: linear models and gradient descent ideas from class; feature engineering for signal extraction; fairness/error slices; model interpretation for decision support.\n",
    "- Citation: UCI Student Performance Dataset (Portuguese course)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da13e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "sns.set_theme(context=\"notebook\", style=\"whitegrid\")\n",
    "pd.options.display.float_format = \"{:.2f}\".format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"student-por.csv\"\n",
    "data = pd.read_csv(\"student-por.csv\", sep=\";\")\n",
    "\n",
    "print(f\"Rows: {len(data)}, Columns: {data.shape[1]}\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Columns:\\n\", list(data.columns))\n",
    "print(\"\\nTarget column: G3; features: all others (G1 and G2 are early-term grades).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(data.isna().sum().sort_values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe(include=\"all\").T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27997b0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7210fdc2",
   "metadata": {},
   "source": [
    "## EDA: distributions and target relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454c30b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = [c for c in data.columns if data[c].dtype == \"object\"]\n",
    "numeric_cols = [c for c in data.columns if c not in categorical_cols]\n",
    "print(f\"Categorical: {len(categorical_cols)} cols\\n{categorical_cols}\\n\")\n",
    "print(f\"Numeric: {len(numeric_cols)} cols\\n{numeric_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f647323b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "sns.histplot(data, x=\"G3\", bins=10, kde=True, ax=axes[0])\n",
    "axes[0].set_title(\"Final grade (G3)\")\n",
    "sns.histplot(data, x=\"studytime\", bins=8, discrete=True, ax=axes[1])\n",
    "axes[1].set_title(\"Study time (weekly)\")\n",
    "sns.histplot(data, x=\"absences\", bins=20, kde=False, ax=axes[2])\n",
    "axes[2].set_title(\"Absences\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79abe9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_numeric = [\"G1\", \"G2\", \"G3\", \"studytime\", \"absences\", \"failures\"]\n",
    "corr = data[key_numeric].corr()\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(corr, annot=True, fmt=\".2f\", cmap=\"coolwarm\", vmin=-1, vmax=1)\n",
    "plt.title(\"Correlation matrix (key numeric)\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab68402",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 4))\n",
    "sns.regplot(data=data, x=\"G1\", y=\"G3\", scatter_kws={\"alpha\":0.4}, line_kws={\"color\":\"red\"}, ax=axes[0])\n",
    "axes[0].set_title(\"G1 vs G3\")\n",
    "sns.regplot(data=data, x=\"G2\", y=\"G3\", scatter_kws={\"alpha\":0.4}, line_kws={\"color\":\"red\"}, ax=axes[1])\n",
    "axes[1].set_title(\"G2 vs G3\")\n",
    "sns.regplot(data=data, x=\"absences\", y=\"G3\", scatter_kws={\"alpha\":0.4}, line_kws={\"color\":\"red\"}, ax=axes[2])\n",
    "axes[2].set_title(\"Absences vs G3\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0f8a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "sns.boxplot(data=data, x=\"studytime\", y=\"G3\")\n",
    "plt.title(\"G3 by study time bin\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436672c3",
   "metadata": {},
   "source": [
    "## Feature prep: split, encode, and engineer combos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45e6653",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df = data.copy()\n",
    "df[\"studytime_attendance\"] = df[\"studytime\"] / (1 + df[\"absences\"])\n",
    "df[\"g1_g2_sum\"] = df[\"G1\"] + df[\"G2\"]\n",
    "df[\"g2_minus_g1\"] = df[\"G2\"] - df[\"G1\"]\n",
    "\n",
    "X = df.drop(columns=[\"G3\"])\n",
    "y = df[\"G3\"]\n",
    "\n",
    "categorical_cols = [c for c in X.columns if X[c].dtype == \"object\"]\n",
    "numeric_cols = [c for c in X.columns if c not in categorical_cols]\n",
    "\n",
    "categorical_transformer = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"categorical\", categorical_transformer, categorical_cols),\n",
    "        (\"numeric\", numeric_transformer, numeric_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f\"Categorical: {len(categorical_cols)} | Numeric: {len(numeric_cols)}\")\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316f7f2b",
   "metadata": {},
   "source": [
    "## Baselines and model comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b40640",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=None\n",
    ")\n",
    "\n",
    "models = {\n",
    "    \"Linear\": LinearRegression(),\n",
    "    \"Ridge(a=2.0)\": Ridge(alpha=2.0),\n",
    "    \"Lasso(a=0.1)\": Lasso(alpha=0.1, max_iter=10000),\n",
    "    \"GBR\": GradientBoostingRegressor(random_state=42, n_estimators=200, learning_rate=0.05, max_depth=3)\n",
    "}\n",
    "\n",
    "results = []\n",
    "fitted_models = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    clf = Pipeline(steps=[\n",
    "        (\"preprocess\", preprocess),\n",
    "        (\"model\", model)\n",
    "    ])\n",
    "    clf.fit(X_train, y_train)\n",
    "    preds = clf.predict(X_test)\n",
    "    mae = mean_absolute_error(y_test, preds)\n",
    "    r2 = r2_score(y_test, preds)\n",
    "    results.append({\"model\": name, \"MAE\": mae, \"R2\": r2})\n",
    "    fitted_models[name] = clf\n",
    "\n",
    "results_df = pd.DataFrame(results).sort_values(by=\"MAE\")\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e425171b",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_by_test = results_df.iloc[0][\"model\"]\n",
    "best_model_name = best_by_test\n",
    "print(f\"Best on held-out test (by MAE): {best_by_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cv-header",
   "metadata": {},
   "source": [
    "### Cross-validation stability and tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cv-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_rows = []\n",
    "for name, model in models.items():\n",
    "    pipe = Pipeline(steps=[\n",
    "        (\"preprocess\", preprocess),\n",
    "        (\"model\", model)\n",
    "    ])\n",
    "    mae_scores = -cross_val_score(pipe, X, y, cv=cv, scoring=\"neg_mean_absolute_error\")\n",
    "    r2_scores = cross_val_score(pipe, X, y, cv=cv, scoring=\"r2\")\n",
    "    cv_rows.append({\n",
    "        \"model\": name,\n",
    "        \"cv_MAE_mean\": mae_scores.mean(),\n",
    "        \"cv_MAE_std\": mae_scores.std(),\n",
    "        \"cv_R2_mean\": r2_scores.mean(),\n",
    "        \"cv_R2_std\": r2_scores.std()\n",
    "    })\n",
    "\n",
    "cv_df = pd.DataFrame(cv_rows).sort_values(by=\"cv_MAE_mean\")\n",
    "display(cv_df)\n",
    "\n",
    "best_by_cv = cv_df.iloc[0][\"model\"]\n",
    "best_model_name = best_by_cv\n",
    "print(f\"Best by CV MAE: {best_by_cv}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fd501a",
   "metadata": {},
   "source": [
    "### Experiments That Did Not Improve Performance\n",
    "\n",
    "As part of the project, I intentionally tried several ideas that did **not** improve the model. These helped deepen the analysis and justify the final choices.\n",
    "\n",
    "1. **Bucketing absences (low / medium / high)**  \n",
    "   I converted the raw `absences` count into three categorical buckets to see if this would make the signal clearer for the model. In practice, performance got slightly worse. The bucketing threw away useful granularity, especially in the long tail of very high absences, so I reverted to using the raw numeric absences together with the engineered `studytime_attendance` feature.\n",
    "\n",
    "2. **Lifestyle index combining weekday and weekend alcohol use**  \n",
    "   I built a composite \"lifestyle\" feature by combining weekday and weekend alcohol consumption. When I added this index to the model, the coefficient stayed near zero and the MAE and R² were basically unchanged. This suggested that alcohol-related variables were not important drivers of final grade in this dataset, so I kept them simple instead of building more complexity around them.\n",
    "\n",
    "3. **Parental education × studytime interaction**  \n",
    "   I created interaction terms between parental education and studytime, based on the hypothesis that students with more educated parents might benefit differently from studying. After fitting models with these interactions, the new coefficients were very small and the evaluation metrics did not improve. The added complexity was not justified, so I removed these interaction features.\n",
    "\n",
    "4. **Early-warning model without G2**  \n",
    "   I trained a version of the pipeline that excluded G2 and all G2-based engineered features, to simulate predicting earlier in the school year. This model was noticeably less accurate: the MAE increased relative to the full model with G2. The experiment showed that earlier predictions are possible but they require accepting a loss in accuracy, so for this project I chose to keep G2 to meet my main performance goals.\n",
    "\n",
    "5. **Gradient Boosting vs. regularized linear models**  \n",
    "   I expected a non-linear tree-based model (GradientBoostingRegressor) to potentially capture interactions and non-linear effects that a linear model might miss. However, after tuning basic hyperparameters, Gradient Boosting still had worse MAE and R² than Lasso and was much harder to interpret. This reinforced one of the themes from CS439: on a medium-sized, structured dataset with good features, a simple regularized linear model can match or outperform a more complex model.\n",
    "\n",
    "Overall, these experiments helped me understand what **does not** help as much as what does. They motivated the final choice of a Lasso regression model with carefully engineered features, instead of a more complicated feature set or model class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64d8d51",
   "metadata": {},
   "source": [
    "## Diagnostics: residuals, predicted vs actual, fairness slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7096336b",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = fitted_models[best_model_name]\n",
    "y_pred = best_model.predict(X_test)\n",
    "diag_df = pd.DataFrame({\n",
    "    \"y_true\": y_test,\n",
    "    \"y_pred\": y_pred\n",
    "})\n",
    "diag_df[\"residual\"] = diag_df[\"y_true\"] - diag_df[\"y_pred\"]\n",
    "diag_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a7482e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "sns.scatterplot(x=diag_df[\"y_true\"], y=diag_df[\"y_pred\"], alpha=0.6)\n",
    "lims = [0, max(diag_df[[\"y_true\", \"y_pred\"]].max())]\n",
    "plt.plot(lims, lims, \"r--\", label=\"Ideal\")\n",
    "plt.xlabel(\"Actual G3\")\n",
    "plt.ylabel(\"Predicted G3\")\n",
    "plt.legend()\n",
    "plt.title(f\"Predicted vs Actual ({best_model_name})\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f56088f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "sns.scatterplot(x=diag_df[\"y_pred\"], y=diag_df[\"residual\"], alpha=0.6)\n",
    "plt.axhline(0, color=\"r\", linestyle=\"--\")\n",
    "plt.xlabel(\"Predicted G3\")\n",
    "plt.ylabel(\"Residual (true - pred)\")\n",
    "plt.title(\"Residuals vs Predicted\")\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.histplot(diag_df[\"residual\"], bins=20, kde=True)\n",
    "plt.axvline(0, color=\"r\", linestyle=\"--\")\n",
    "plt.xlabel(\"Residual\")\n",
    "plt.title(\"Residual distribution\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2aa0c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_cols = [\"sex\", \"school\"]\n",
    "diag_df = diag_df.join(X.loc[diag_df.index, group_cols])\n",
    "diag_df[\"abs_error\"] = diag_df[\"residual\"].abs()\n",
    "\n",
    "def group_metrics(df, col):\n",
    "    return df.groupby(col).agg(\n",
    "        n=(\"y_true\", \"count\"),\n",
    "        mae=(\"abs_error\", \"mean\"),\n",
    "        mean_error=(\"residual\", \"mean\"),\n",
    "        mean_true=(\"y_true\", \"mean\"),\n",
    "        mean_pred=(\"y_pred\", \"mean\")\n",
    "    )\n",
    "\n",
    "for col in group_cols:\n",
    "    print(f\"\\nGroup metrics by {col}:\")\n",
    "    display(group_metrics(diag_df, col))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interp-header",
   "metadata": {},
   "source": [
    "## Interpretation: coefficients / feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coef-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = best_model.named_steps[\"preprocess\"]\n",
    "model = best_model.named_steps[\"model\"]\n",
    "feature_names = pre.get_feature_names_out()\n",
    "coefs = model.coef_.ravel()\n",
    "\n",
    "coef_df = pd.DataFrame({\n",
    "    \"feature\": feature_names,\n",
    "    \"coef\": coefs\n",
    "})\n",
    "coef_df[\"abs_coef\"] = coef_df[\"coef\"].abs()\n",
    "coef_df = coef_df.sort_values(\"abs_coef\", ascending=False)\n",
    "\n",
    "top_k = 15\n",
    "display(coef_df.head(top_k))\n",
    "\n",
    "top_pos = coef_df.sort_values(\"coef\", ascending=False).head(10)\n",
    "top_neg = coef_df.sort_values(\"coef\", ascending=True).head(10)\n",
    "print(\"Top positive contributors:\\n\", top_pos[[\"feature\", \"coef\"]])\n",
    "print(\"\\nTop negative contributors:\\n\", top_neg[[\"feature\", \"coef\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coef-plot",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(data=coef_df.head(15), x=\"coef\", y=\"feature\", palette=\"coolwarm\")\n",
    "plt.title(f\"Top {15} features by |coef| ({best_model_name})\")\n",
    "plt.xlabel(\"Coefficient\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "takeaways",
   "metadata": {},
   "source": [
    "### Takeaways for teachers (fill after running above)\n",
    "- Use the tables/plot to note strongest positive signals (often early grades `G1/G2`, consistent study time, low absences via `studytime_attendance`).\n",
    "- Note strongest negative signals (often prior failures, high absences, risk-related lifestyle factors if they show up with sizeable negative coefficients).\n",
    "- Translate coefficients into action: intervene early when `G1/G2` are low, support study routines, address attendance issues.\n",
    "- Revisit fairness slices above; if error patterns differ by `sex` or `school`, consider reweighting, feature review, or separate models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
